{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"environment":{"kernel":"conda-env-tensorflow-tensorflow","name":"workbench-notebooks.m113","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/workbench-notebooks:m113"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":51294,"databundleVersionId":7331882,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jeffreyesedo/1st-ribo-note?scriptVersionId=262667765\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/jeffreyesedo/1st-ribo-note?scriptVersionId=151222394\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"# RNA Science Environment and Libraries","metadata":{}},{"cell_type":"code","source":"# Setting up an RNA Science Environment\n!pip install arnie\n!pip install draw_rna\n# !pip install viennarna\n!pip install swifter\n\n\n# Install EternaFold\n!conda config --set auto_update_conda false\n!conda install -c bioconda eternafold --yes\n\n# Install ViennaRNA with conda for arnie\n# The provided output shows you installed it with pip, but conda is generally\n# more reliable for setting up the binaries arnie needs.\n!conda install viennarna -c bioconda --yes\n\n# Manually setup EternaFold and Vienna for Kaggle notebook\n%env ETERNAFOLD_PATH=/opt/conda/bin/eternafold-bin\n%env ETERNAFOLD_PARAMETERS=/opt/conda/lib/eternafold-lib/parameters/EternaFoldParams.v1\n%env VIENNA_2_PATH = /opt/conda/bin\n%env TMP=/content/tmp","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!which RNAfold","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!which viennarna","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport psutil\nimport gc\nimport random\nimport ast\nimport swifter\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport glob as glob\n\n\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm, trange\nfrom time import sleep","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import RNA\nimport arnie\nfrom arnie.mfe import mfe\nfrom arnie.bpps import bpps\nfrom arnie.pfunc import pfunc\nimport arnie.utils as utils\nfrom arnie.free_energy import free_energy\nfrom draw_rna.ipynb_draw import draw_struct","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import Datasets","metadata":{}},{"cell_type":"markdown","source":"## train data","metadata":{}},{"cell_type":"code","source":"train= pd.read_csv(\"/kaggle/input/stanford-ribonanza-rna-folding/train_data.csv\")\n\nprint(f\"Train dataset shape: {train.shape}\\n\")","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## optimizing dataset for memory","metadata":{}},{"cell_type":"code","source":"def opt_num(df):\n    # optimize numerical data columns\n    df= df.copy()\n    \n    for col in df.columns:\n        df_col= df[col]\n        dn = df_col.dtype.name\n        \n        if dn == \"int64\":\n            df[col]= pd.to_numeric(df_col, downcast=\"integer\")\n        elif dn == \"float64\":\n            df[col]= pd.to_numeric(df_col, downcast=\"float\")\n        elif dn == \"object\":\n            num_unique_values = len(df_col.unique())\n            num_total_values = len(df_col)\n            if num_unique_values / num_total_values < 0.5:\n                df[col] = df_col.astype(\"category\")\n    return df","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt_train= opt_num(train)","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Train Dataset:{train.iloc[0:5, 0:10].info()}\")\nprint()\nprint()\nprint(f\"Optimized Dataset: {opt_train.iloc[0:5, 0:10].info()}\")","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del train\ngc.collect()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Exploration and Visualization","metadata":{}},{"cell_type":"code","source":"opt_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count columns based on their Dtype\n# dtype_counts = opt_train.dtypes.value_counts()\n# print(dtype_counts)","metadata":{"tags":[],"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"experiments_count= opt_train[\"experiment_type\"].value_counts()\nprint(experiments_count)","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rna_viz(data, experiment, package):\n    # Visualizing RNA  sequence base on experiment\n    exp_type= data[data.experiment_type == experiment]\n    seq_index= random.randint(0,len(exp_type.sequence))\n    \n    seq_exp = opt_train[data[\"experiment_type\"] == experiment].iloc[seq_index, 1:3]\n    print(seq_exp)\n    \n    structure = mfe(seq_exp.sequence,package=package)\n    print(structure)\n    \n    fig, axs = plt.subplots(1,1,  figsize=(8,7))\n    draw_struct(seq_exp.sequence, structure, ax=axs)\n    axs.set_title(seq_exp.experiment_type, loc='left', fontsize='medium')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualizing RNA  sequence for DMS MaP\nrna_viz(opt_train, \"DMS_MaP\",  \"eternafold\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualizing RNA  sequence for 2A3 MaP\nrna_viz(opt_train, \"2A3_MaP\",  \"eternafold\")","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seq_len= opt_train.sequence.apply(len)\nseq_len = seq_len.value_counts()\n# seq_len = pd.Series(seq_len)\n# seq_len\n\nseq_len.plot.bar()\nplt.xlabel(\"Sequences Lenght\")\nplt.title(\"Sequence lenght Distribution\")","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"lengths of RNA sequence is between 115 to 206, while for the test the lengths are between 177 to 457.  Part of the challenge is to know whether the patterns recognized at length 115 to 206 will generalize to longer lengths [response found here.](https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/discussion/453147#2513582).","metadata":{}},{"cell_type":"code","source":"base= {\"A\": 0,\"C\":0,\"G\":0,\"U\":0}\n\nfor seq in opt_train.sequence:\n    for base_key in base.keys():\n        base[base_key] += seq.count(base_key)\n\n\nplt.bar(base.keys(), base.values())\nplt.xlabel('Base', fontsize = 12, fontweight = 'bold', color = 'darkblue')\nplt.ylabel('Count', fontsize = 12, fontweight = 'bold', color = 'darkblue')\nplt.title('Base Count', fontsize = 14, fontweight = 'bold', color = 'darkgreen')","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del seq_len\ngc.collect()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt_train.reads.describe()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt_train.signal_to_noise.describe()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt_train.SN_filter.describe()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking number of columns NaN in reactivity and reactivity_error \nfloat_columns = opt_train.select_dtypes(include=['float'])\n\n# Columns that are NaN\nnum_empty_cols= 0\ncols_having_values=0\n\n\n# for col in float_columns.drop('signal_to_noise', axis=1):\nfor col in float_columns:\n    if float_columns[col].notna().sum() == 0:\n        num_empty_cols+=1\n    else:\n        cols_having_values+=1\n        \nprint(f\"Number of Columns with only NaN values: {num_empty_cols} of 412 columns\\n\")\nprint(f\"Number of Columns with values: {cols_having_values} of 412 columns\")","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del float_columns\ngc.collect()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Wrangling","metadata":{}},{"cell_type":"code","source":"def wrangle(df):\n    \n    # Drop rows based on SN Filter\n    df= df.loc[df.SN_filter == 1]\n    \n    # Drop duplicate\n    df= df.drop_duplicates(subset=[\"sequence_id\", \"experiment_type\"])\n    \n    \n    # Drop the columns \n    df= df.drop(columns=[\"reads\", \"signal_to_noise\",\"SN_filter\"], axis=1)\n    df= df.drop(columns=[col for col in df.columns if \"_error_\" in col], axis=1)\n  \n    # Set categories for categorical columns\n    for col in df.select_dtypes(include=\"category\"):\n        df[col] = df[col].cat.add_categories([0])\n    \n    \n    # Fill NaN value for reactivity & error\n    df= df[7:].fillna(0)\n    \n    # Reset index to start from 0\n    df.reset_index(drop=True, inplace= True)\n       \n      \n    return df","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get the clean dataset\ntrain_feat=  wrangle(opt_train)","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_feat.head()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_feat.info()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del opt_train\ngc.collect()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction and Engineering\n<!-- - Mean of Bpps -->\n<!-- - 3D Coords -->\n<!-- - Sequence lib -->\n<!-- - forming OpenKnots and the probabity using metadata -->\n<!-- - Probability codons  -->\n<!-- - Mean of probability of codons -->\n<!-- - propbability of forming 2D and 3D structures -->\n<!-- - sequence length -->\n<!-- - Mean reactivity -->\n<!-- - secondary structure and its' count [UMAR IGAN](https://www.kaggle.com/code/umar47/rna-folding-reduce-memory-add-features-seq2seq?scriptVersionId=147271807&cellId=31) -->\n<!-- - Adjacent Guanines count -->","metadata":{}},{"cell_type":"markdown","source":"### energetics and structure \nUsing the arnie package, to get the following features.\n- Bpps Thank to [JOCELYN DUMLAO](https://www.kaggle.com/jocelyndumlaohttps://www.kaggle.com/jocelyndumlao)\n- Dot-bracket notation\n- Free energy\n- pair and unpaired vector","metadata":{}},{"cell_type":"code","source":"# funcitons to get energitcis and structure data\n\ndef energtics_structure_parallel(sequence, package):\n    \"\"\"Get the secondary features for an RNA sequence, \n    derived using arnie and eternafold packages\n    \n    Parameters\n    ----------\n    sequence: str\n        sequence of bases for an RNA\n    \n    Returns\n    -------\n    features: DataFrame\n    - Minimum free energy (dot notation)\n    - free energy \n    \"\"\"\n    def process_seq(seq):\n        seq_info = {}\n\n        seq_info[\"mfe\"] = mfe(seq, package)\n        seq_info[\"free_energy\"] = free_energy(seq, package)\n\n        return seq_info\n\n    with ThreadPoolExecutor() as executor:\n        struc_energ = list(executor.map(process_seq, sequence))\n\n\n    df = pd.DataFrame(struc_energ)\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_data_in_batches(data, batch_size, name, package):\n    \"\"\"Process large RNA sequence data in batches\n        \n    Parameters\n    ----------\n    data: str\n        List of RNA sequence data\n    \n    batch_size: int\n        size or number of rows for each batch\n    \n    name: str\n        name for processed dataset\n\n    package: str\n        name of model\n    \n    Returns\n    -------\n        creates csv files for each processed batch\n    \"\"\"\n    # Get batches\n    total_count = len(data)\n    chunks = (total_count - 1) // batch_size + 1 \n        \n    \n    with tqdm(total=chunks, desc=\"Processing Batches\") as pbar: # progress bar for batch processing\n        sleep(0.1)\n        for i in range(chunks):\n            batch = data[i * batch_size: (i + 1) * batch_size]\n            \n            filename = f'{name}_struc_{i + 1}.csv'\n            feat_path= glob.glob(\"/kaggle/input/features-data/*.csv\")\n            \n        \n            if os.path.exists(filename) or filename in feat_path: \n                last_row = pd.read_csv(filename).iloc[-1].tolist()\n            else:\n                energtics_structure_parallel(batch, package).to_csv(filename)\n                \n            pbar.update(1)  # Increment the progress \n\n        # Cleanup: remove all .ps files\n        for file in glob.glob(\"/kaggle/working/*.ps\"):\n            os.remove(file)\n            print(f\"Removed temporary file: {file}\")\n        \n        # Release unreferenced memory\n        del batch\n        gc.collect()\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get mfe and free energy\ndata= train_feat.sequence[:100]\nbatch_size= 10\nname= \"train_mfe_free_e\"\npackage= \"eternafold\"\nprocess_data_in_batches(data, batch_size, name, package)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read the structure csv file into pandas\npd.read_csv(\"/kaggle/working/train_mfe_free_e_struc_1.csv\").head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get bpps and saving using np.save\n\ndef get_bpps(seqs, filename, package):\n    \"\"\"\n    Gets base pair probability matrices and saves them in .npy format.\n\n    Args:\n        seqs (list): A list of RNA sequences.\n        filename (str): The name of the file to save the data.\n    \"\"\"\n    bp_matrices = []\n    for seq in tqdm(seqs, desc=\"Processing\", unit=\"sequence\"):\n        # Assuming bpps() returns a NumPy array\n        bp_matrices.append(bpps(seq, package=package))\n    \n    # Save the list of arrays as a single .npy file\n    np.save(filename, bp_matrices)\n    print(f\"Successfully saved {len(bp_matrices)} matrices to {filename}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# batch processing for bp matrix\ndef process_data_in_batches(data, batch_size, name, package):\n    \"\"\"Process large RNA sequence data in batches\n        \n    Parameters\n    ----------\n    data: str\n        List of RNA sequence data\n    \n    batch_size: int\n        size or number of rows for each batch\n    \n    name: str\n        name for processed dataset\n\n    package: str\n        name of model\n    \n    Returns\n    -------\n        creates csv files for each processed batch\n    \"\"\"\n    # Get batches\n    total_count = len(data)\n    chunks = (total_count - 1) // batch_size + 1 \n        \n    \n    with tqdm(total=chunks, desc=\"Processing Batches\") as pbar: # progress bar for batch processing\n        sleep(0.1)\n        for i in range(chunks):\n            batch = data[i * batch_size: (i + 1) * batch_size]\n            \n            filename = f'{name}_struc_{i + 1}.csv'\n            feat_path= glob.glob(\"/kaggle/input/features-data/*.csv\")\n            \n            \"\"\"change this code to handle arrays\"\"\"\n            # if os.path.exists(filename) or filename in feat_path: \n            #     last_row = pd.read_csv(filename).iloc[-1].tolist()\n            # else:\n            #     energtics_structure_parallel(batch, package).to_csv(filename)\n                \n            pbar.update(1)  # Increment the progress \n\n        # Cleanup: remove all .ps files\n        for file in glob.glob(\"/kaggle/working/*.ps\"):\n            os.remove(file)\n            print(f\"Removed temporary file: {file}\")\n        \n        # Release unreferenced memory\n        del batch\n        gc.collect()\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_bpps(train_feat.sequence, \"bp_matrix.npy\", \"eternafold\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the data back into a list of NumPy arrays\nbp_matrix = np.load(\"bp_matrix.npy\")\n\nprint(f\"Loaded a list of {len(bp_matrix)} matrices.\")\nprint(f\"Shape of array {len(bp_matrix.shape)}.\")\nprint(f\"Shape of the first matrix: {bp_matrix[0].shape}\")\nprint(bp_matrix[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read into pandas\nbp_matrix= pd.Series({\"bp_matrix\": bp_matrix})\n\nbp_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Data type\", type(bp_matrix.bp_matrix[0]))\nprint(\"\\n\")\nprint(\"\\n\")\nbp_matrix.bp_matrix[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize a sample matrix\nbp_m= bp_matrix.bp_matrix[0]\nplt.imshow(bp_m, origin='lower', cmap='gist_heat_r')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get the total probability that nucleotide i is paired for each sequence\n# This gives us a prediction of how accessible the nucleotide is, which is important in a lot of contexts -- \n# a prediction for structure probing data, a prediction for degradation rate, and possibly a prediction for protein binding.\n\ndef get_p_unp_vec(bp_matrix):\n    p_unp_vec= {}\n    p_unp_vec[\"p_unp_vec\"] = 1 - np.sum(bp_matrix, axis=0)\n    p_unp_vec= pd.DataFrame(p_unp_vec, columns= [\"p_unp_vec\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get a slice 20 000 of the train data\ntrain_sliced= train_feat[:10_000]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%time\n# Get the Secondary features for train and test datasets\nprocess_data_in_batches(train_feat.sequence, 50_000, \"train\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.read_csv(\"/kaggle/input/features-data/train_sec_struc_1.csv\").head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# concatenating csv files into a csv file for test and training data\n\ndef csv_concat(file_path, name):\n    \"\"\" Combine all the csv files into one file\n    \n    Parameter\n    ---------\n    file_path: str\n        path to the csv files\n    \n    name: str\n        name of combined csv file\n    \n    \n    Returns\n    -------\n    combined csv file\n    \n    \"\"\"\n\n    # Create a list of CSV files  to append\n    file_list = os.listdir(file_path)\n    extract_numeric_part = lambda x: int(x.split('_')[3].split('.')[0])\n    sorted_file_list = sorted(file_list, key=extract_numeric_part)\n\n    # Read each CSV file into a DataFrame\n    combined_csv = pd.concat([pd.read_csv(f\"{file_path}/{f}\") for f in sorted_file_list], ignore_index=True)\n\n    # Export the combined DataFrame to a single CSV file\n    combined_csv.to_csv(f\"features_data/{name}.csv\", index=False)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# download the features data for test and trian\n\n# !gsutil -m cp \\\n# !  \"gs://kaggle-competition-402916-eu-notebooks/europe-west2-a/instance-20231122-222833/Kaggle-Competition----Stanford-Ribonanza-RNA-Folding/features_data/combined_features/combine_test.csv\" \\\n# !  \"gs://kaggle-competition-402916-eu-notebooks/europe-west2-a/instance-20231122-222833/Kaggle-Competition----Stanford-Ribonanza-RNA-Folding/features_data/combined_features/combine_train.csv\" \\\n\n\n# ! wget https://storage.cloud.google.com/kaggle-competition-402916-eu-notebooks/europe-west2-a/instance-20231122-222833/Kaggle-Competition----Stanford-Ribonanza-RNA-Folding/features_data/combined_features/combine_test.csv?_ga=2.10857908.-1088134625.1698079045\n# ! wget https://storage.cloud.google.com/kaggle-competition-402916-eu-notebooks/europe-west2-a/instance-20231122-222833/Kaggle-Competition----Stanford-Ribonanza-RNA-Folding/features_data/combined_features/combine_train.csv?_ga=2.10857908.-1088134625.1698079045    ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Concatenate the energetics and structure csv files\n\ncsv_concat(\"train_features\", \"combine_train\")\n\ncsv_concat(\"test_features\", \"combine_test\")\n\n\ncombine_train_features= pd.read_csv(\"combine_train.csv\")\ncheck.shape\ncombine_test_features= pd.read_csv(\"combine_test.csv\")\ncheck.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import structure and bpps to data\n\ntrain_struc_bpps= pd.read_csv(\"train_struc_bpps.csv\")\ntest_struc_bpps= pd.read_csv(\"test_struc_bpps.csv\")\n\nprint(f\"train extracted features shape: {test_struc_bpps.shape}\\ntest extracted features shape {\"test_struc_bpps\"}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### sequence lenght","metadata":{}},{"cell_type":"code","source":"# lenght of sequence to a column\ntrain_feat[\"sequnece_len\"]= train_feat.sequence.astype(str).apply(len)\nopt_test[\"sequnece_len\"]= opt_test.sequence.apply(len)\n\ntrain_feat","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opt_test.info()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### mean reactivity","metadata":{}},{"cell_type":"code","source":"# Get the mean of reactivity columns \ntrain_feat[\"react_mean\"]= train_feat[reactivity_cols].mean(axis=1)\ntrain_feat","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate mean BPPs\n\ntrain_struc_bpps[\"avg_bpps\"]= train_seq_bpp.mean(axis=1)\ntest_struc_bpps[\"avg_bpps\"] = test_seq_bpp.mean(axis=1)\n\n\n# print(f\"Train dataset shape: {train_struc_bpps}\")\n# print(f\"Test dataset shape: {test_struc_bpps}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to count parentheses\ndef count_parentheses(structure_string):\n    count = structure_string.count(\")\")\n    return count\n\n# Apply the function to the DataFrame column\n\ntq.pandas()\ntrain_struc_bpps['parentheses_counts'] = train_struc_bpps['sec_structure'].astype(str).apply(count_parentheses)\ntest_struc_bpps['parentheses_counts'] = test_struc_bpps['sec_structure'].astype(str).apply(count_parentheses)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### codon features\ncodon count, cps of sequence, codon probability","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ndef codons_feats(seq):\n    codons = Counter(seq[i:i+3] for i in range(0, len(seq), 3))\n    pairs = Counter(seq[i:i+6] for i in range(0, len(seq)-1, 3))\n    cps = 0\n    for pair in pairs:\n        if codons[pair[:3]] == 0 or codons[pair[3:]] == 0:\n            continue\n        cps += pairs[pair]/(codons[pair[:3]]*codons[pair[3:]])\n    return {'codons': codons, 'pairs': pairs, 'cps': cps}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the codons, pairs, cps for train sequences\n# codon_feat= []\n\n# for seq in train_feat.sequence:\n#     codon_feat.append(codons_feats(seq))\n\n# # codon_feat= pd.DataFrame(train_feat.sequence.iloc[5:10].apply(codons_feats), columns= [\"codons\", \"pairs\", \"cps\"])\n# codon_feat= pd.DataFrame(codon_feat, columns=[\"codons\", \"pairs\", \"cps\"])\n# codon_feat.to_csv(\"train_codon_feat.csv\")\n\n# codon_feat.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the codons, pairs, cps for test sequences\n# codon_feat= []\n\n# for seq in opt_test.sequence:\n#     codon_feat.append(codons_feats(seq))\n\n# # codon_feat= pd.DataFrame(train_feat.sequence.iloc[5:10].apply(codons_feats), columns= [\"codons\", \"pairs\", \"cps\"])\n# codon_feat= pd.DataFrame(codon_feat, columns=[\"codons\", \"pairs\", \"cps\"])\n# codon_feat.to_csv(\"test_codon_feat.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define functionto calculate codon probabiltiy\ndef codon_probs_mean(seq):\n#     probs = seq.apply(RNA.codon_prob)\n    probs = [RNA.codon_prob(seq) for s in seq]\n    mean_probs= sum(probs.values()) / len(probs)\n    probs_mean= pd.DataFrame({\"probs\":probs, \"mean_probs\": mean_probs})\n    \n    return probs_mean\n\n\ntrain_condon_probs_mean= codon_probs_mean(train_feat.sequence[:10])\ntrain_condon_probs_mean\n# train_condon_probs_mean.to_csv(\"train_condon_probs_mean.csv\")\n\n# test_condon_probs_mean= codon_probs_mean(opt_test.sequence)\n# test_condon_probs_mean.to_csv(\"test_condon_probs_mean.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### count of adjecent guanines in sequence","metadata":{}},{"cell_type":"code","source":"# function to count adjacent guanines in a codon\n\ndef gg_count(seq):\n    \"\"\"\n    Returns:\n    list of adjcent gg or ggg counts for each sequence\n    \"\"\"\n    adj_guanine= []\n    # Count the number of adjacent guanines\n    for s in seq:\n    adj_guanine.append(gg_count = 0)\n    for i in range(len(s) - 1):\n        if s[i:i+2] == \"GG\" or \"GGG\":\n            gg_count += 1\n            \n    return gg_seq_num\n\n\ntrain_struc_bpps[\"adj_guanine\"]= gg_count(train_feat.sequence)\ntest_struc_bpps[\"adj_guanine\"]= gg_count(opt_test.sequence)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### concatenate features into datasets","metadata":{}},{"cell_type":"code","source":"# Concatenate All features to one Dataset called features and test_features\n\nfeatures= pd.concat([train_feat,train_struc_bpps, train_condon_probs_mean])\ntest_features= pd.concat([opt_test,test_struc_bpps,test_condon_probs_mean])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del train_feat\ndel opt_test\ndel train_struc_bpps\ndel test_struc_bpps\ndel train_condon_probs_mean\ndel test_condon_probs_mean\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## secondary structures data","metadata":{}},{"cell_type":"code","source":"# Import eterna openknot dataset\n# eterna_pos= pd.read_table(\"/kaggle/input/stanford-ribonanza-rna-folding/eterna_openknot_metadata/Positives240-2000.tsv\", sep=\"\\\\t\")\n# eterna_puz_132= pd.read_csv(\"/kaggle/input/stanford-ribonanza-rna-folding/eterna_openknot_metadata/puzzle 12378132.tsv\", sep= \"\\\\t\")\n# eterna_puz_RYOP50= pd.read_csv(\"/kaggle/input/stanford-ribonanza-rna-folding/eterna_openknot_metadata/puzzle_11318423_RYOP50_with_description.tsv\", sep= \"\\\\t\")\n# eterna_puz_RYOP90= pd.read_csv(\"/kaggle/input/stanford-ribonanza-rna-folding/eterna_openknot_metadata/puzzle_11387276_RYOP90_with_description.tsv\", sep= \"\\\\t\")\n# eterna_puz_RFAM= pd.read_csv(\"/kaggle/input/stanford-ribonanza-rna-folding/eterna_openknot_metadata/puzzle_11627601_with_descriptions_PLUS_RFAM.tsv\", sep= \"\\\\t\")\n# eterna_puz_118= pd.read_csv(\"/kaggle/input/stanford-ribonanza-rna-folding/eterna_openknot_metadata/puzzle_11836497_with_description.tsv\", sep= \"\\\\t\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Import Supplementary Silico prediction, that is, secondary structure predictions\n# gpn15k_preds= pd.read_csv(\"/kaggle/input/stanford-ribonanza-rna-folding/supplementary_silico_predictions/GPN15k_silico_predictions.csv\")\n# pk50_preds= pd.read_csv(\"/kaggle/input/stanford-ribonanza-rna-folding/supplementary_silico_predictions/PK50_silico_predictions.csv\")\n# pk90_preds= pd.read_csv(\"/kaggle/input/stanford-ribonanza-rna-folding/supplementary_silico_predictions/PK90_silico_predictions.csv\")\n# r1_preds= pd.read_csv(\"/kaggle/input/stanford-ribonanza-rna-folding/supplementary_silico_predictions/R1_silico_predictions.csv\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gpn15k_preds.shape\n# gpn15k_preds.head()\n# pk50_preds.shape\n# pk50_preds.head()\n# pk90_preds.shape\n# pk90_preds.head()\n# r1_preds.shape\n# r1_preds.head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building Model","metadata":{}},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Dense, Conv1D, Flatten, Input, concatenate\n\n# sequence input (assuming one-hot encoded sequences of length 4)\nsequence_input = Input(shape=(None, 4))\nconv1 = Conv1D(64, kernel_size=3, activation='relu')(sequence_input)\nconv2 = Conv1D(32, kernel_size=3, activation='relu')(conv1)\nflat = Flatten()(conv2)\n\n# numerical/categorical input\nnumerical_input = Input(shape=(4,))\ndense1 = Dense(32, activation='relu')(numerical_input)\n\n# concatenate sequence and numerical inputs\nconcat = concatenate([flat, dense1])\n\n# output layer\noutput = Dense(1, activation='sigmoid')(concat)\n\n# create a model\nmodel = Model(inputs=[sequence_input, numerical_input], outputs=output)\n\n# compile model using MAE as a measure of model performance\nmodel.compile(optimizer='adam', loss='mean_absolute_error')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}